{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# adversatial attack on utut\n",
    "import torch\n",
    "import torchaudio\n",
    "import IPython.display as ipd\n",
    "from torch.optim import Adam\n",
    "import torch.nn.functional as F\n",
    "import tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "os.environ[\"CUDA_VISIBLE_DEVICES\"] = \"1\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Configuration Init"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_lang = \"en\"\n",
    "tgt_lang = \"es\"\n",
    "mhubert_path = \"/root/project/PretrainedModels/utut/mHuBERTBase/mhubert_base_vp_en_es_fr_it3.pt\" \n",
    "kmeans_path = \"/root/project/PretrainedModels/utut/mHuBERTBase/mhubert_base_vp_en_es_fr_it3_L11_km1000.bin\"\n",
    "utut_path = \"/root/project/PretrainedModels/utut/utut_sts.pt\"\n",
    "fr_vocoder_path = \"/root/project/PretrainedModels/utut/vocoder/fr/g_00500000\"\n",
    "en_vocoder_path = \"/root/project/PretrainedModels/utut/vocoder/en/g_00500000\"\n",
    "fr_vocoder_cfg_path = \"/root/project/PretrainedModels/utut/vocoder/fr/config.json\"\n",
    "en_vocoder_cfg_path = \"/root/project/PretrainedModels/utut/vocoder/en/config.json\"\n",
    "in_wav_paths = [\"samples/en/1.wav\",\"samples/en/2.wav\",\"samples/en/3.wav\"]\n",
    "out_wav_paths = [\"samples/es/1.wav\",\"samples/es/2.wav\",\"samples/es/3.wav\"]\n",
    "out_adv_wav_paths = [\"adv_samples/adv/adv_es2en_1.wav\",\"adv_samples/adv/adv_es2en_2.wav\",\"adv_samples/adv/adv_es2en_3.wav\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Test UTUT Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from inference_sts import SpeechToSpeechPipeline\n",
    "from speech2unit.inference import load_model as load_speech2unit_model\n",
    "from unit2unit.inference import load_model as load_unit2unit_model\n",
    "from unit2speech.inference import load_model as load_unit2speech_model\n",
    "from fairseq import utils\n",
    "from util import process_units, save_speech\n",
    "\n",
    "use_cuda = torch.cuda\n",
    "hubert_reader, kmeans_model = load_speech2unit_model(mhubert_path, kmeans_path, use_cuda=use_cuda)\n",
    "fr_task, fr_generator = load_unit2unit_model(utut_path, src_lang, tgt_lang, use_cuda=use_cuda)\n",
    "fr_vocoder = load_unit2speech_model(fr_vocoder_path, fr_vocoder_cfg_path, use_cuda=use_cuda)\n",
    "fr_pipeline = SpeechToSpeechPipeline(\n",
    "        hubert_reader, kmeans_model,\n",
    "        fr_task, fr_generator,\n",
    "        fr_vocoder,\n",
    "        use_cuda=use_cuda\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "in_wav_path = in_wav_paths[0]\n",
    "out_wav_path = out_wav_paths[0]\n",
    "out_adv_wav_path = out_adv_wav_paths[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_unit = fr_pipeline.process_speech2unit(in_wav_path)\n",
    "print(f\"src_unit: {src_unit}\")\n",
    "print(len(src_unit))\n",
    "es_tgt_unit = fr_pipeline.process_unit2unit(src_unit)\n",
    "print(f\"tgt_unit: {es_tgt_unit}\")\n",
    "print(len(es_tgt_unit))\n",
    "tgt_speech = fr_pipeline.process_unit2speech(es_tgt_unit)\n",
    "print(f\"tgt_speech: {tgt_speech}\")\n",
    "save_speech(tgt_speech.detach().cpu().numpy(), out_wav_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# hubert_reader, kmeans_model = load_speech2unit_model(mhubert_path, kmeans_path, use_cuda=use_cuda)\n",
    "en_task, en_generator = load_unit2unit_model(utut_path, src_lang, src_lang, use_cuda=use_cuda)\n",
    "en_vocoder = load_unit2speech_model(en_vocoder_path, en_vocoder_cfg_path, use_cuda=use_cuda)\n",
    "en_pipeline = SpeechToSpeechPipeline(\n",
    "        hubert_reader, kmeans_model,\n",
    "        en_task, en_generator,\n",
    "        en_vocoder,\n",
    "        use_cuda=use_cuda\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "src_unit = en_pipeline.process_speech2unit(in_wav_path)\n",
    "print(f\"src_unit: {src_unit}\")\n",
    "print(len(src_unit))\n",
    "en_tgt_unit = en_pipeline.process_unit2unit(src_unit)\n",
    "en_tgt_unit = process_units(en_tgt_unit)\n",
    "# en_tgt_unit = [int(unit) for unit in en_tgt_unit.split()]\n",
    "print(f\"tgt_unit: {en_tgt_unit}\")\n",
    "print(len(en_tgt_unit))\n",
    "tgt_speech = en_pipeline.process_unit2speech(en_tgt_unit)\n",
    "print(f\"tgt_speech: {tgt_speech}\")\n",
    "save_speech(tgt_speech.detach().cpu().numpy(), \"/root/project/ZY/utut/adv_samples/tmp/adv_en2en_1.wav\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Attack UTUT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tgt_unit_init(pipeline, in_wav_path):\n",
    "    src_unit = pipeline.process_speech2unit(in_wav_path)\n",
    "    tgt_unit = pipeline.process_unit2unit(src_unit)\n",
    "    tgt_unit = process_units(tgt_unit)\n",
    "    tgt_unit_ = [int(unit) for unit in tgt_unit.split()]\n",
    "    tgt_unit_ = tgt_unit_[:100]\n",
    "    return tgt_unit_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torchaudio\n",
    "from torch.optim import Adam\n",
    "from torch.optim.lr_scheduler import StepLR\n",
    "from tqdm import tqdm\n",
    "import torch.nn.functional as F\n",
    "\n",
    "def adversarial_attack(\n",
    "    src_pipeline,\n",
    "    tgt_pipeline,\n",
    "    in_wav_path,\n",
    "    epsilon=0.001,\n",
    "    iterations=100,\n",
    "):\n",
    "    \"\"\"\n",
    "    src_pipeline: src_lang2src_lang pipeline\n",
    "    tgt_pipeline: src_lang2tgt_lang pipeline\n",
    "    \"\"\"\n",
    "    original_tgt_unit_ = tgt_unit_init(src_pipeline, in_wav_path)\n",
    "    original_tgt_unit_tensor = torch.tensor(original_tgt_unit_, dtype = torch.float).detach().cuda()\n",
    "    original_waveform, sr = torchaudio.load(in_wav_path)\n",
    "    original_waveform = original_waveform.cuda()\n",
    "\n",
    "    wave_delta_variable = torch.zeros_like(original_waveform, requires_grad=True).cuda()\n",
    "    optimizer = Adam([wave_delta_variable], lr=epsilon)\n",
    "    scheduler = StepLR(optimizer, step_size=30, gamma=0.1)\n",
    "\n",
    "    for _ in tqdm(range(iterations)):\n",
    "        optimizer.zero_grad()\n",
    "        sample, language, filename = in_wav_path.split('/')\n",
    "        filename = filename.split('.')[0]\n",
    "        in_flag = f\"{sample}_{language}_{filename}\"\n",
    "        temp_path = f\"/root/project/ZY/utut/adv_samples/tmp/{in_flag}_temp_perturbed.wav\"\n",
    "        adversarial_waveform = original_waveform + wave_delta_variable\n",
    "        torchaudio.save(temp_path, adversarial_waveform.detach().cpu(), 16000)  \n",
    "        perturbed_tgt_unit_ = tgt_unit_init(tgt_pipeline, temp_path)\n",
    "        perturbed_tgt_unit_tensor = torch.tensor(perturbed_tgt_unit_, dtype = torch.float).requires_grad_(True).cuda()\n",
    "        \n",
    "        loss = F.cross_entropy(perturbed_tgt_unit_tensor, original_tgt_unit_tensor)\n",
    "        print(f\"loss:{loss.item()}\")\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        wave_delta_variable.data.clamp_(-1, 1)\n",
    "        \n",
    "        scheduler.step()  \n",
    "\n",
    "\n",
    "    return adversarial_waveform.detach()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "out_adv_waveform = adversarial_attack(en_pipeline, fr_pipeline, in_wav_path)\n",
    "save_speech(out_adv_waveform.detach().cpu().numpy(), out_adv_wav_path)\n",
    "ipd.Audio(out_adv_wav_path)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "seamless",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
